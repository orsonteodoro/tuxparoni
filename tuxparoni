#!/bin/bash

# Copyright 2019 Orson Teodoro <orsonteodoro@hotmail.com>
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to
# deal in the Software without restriction, including without limitation the
# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
# sell copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
# DEALINGS IN THE SOFTWARE.

#---

# NVD Data files are in public domain
# Obtained from https://nvd.nist.gov/vuln/data-feeds
# Patches are GPL-2, GPL-2+, or MIT depending on the original file.

#---

# Before using this tool

# Few arguments against using automated patching sample
# 1) It may mispatch; or create duplicate code blocks.
# 2) It may add a hunk that it shouldn't add.
# 3) All hunks need to be inspected manually for mispatches and unnecessary
#    duplicate hunks all for all 1000 patches (with possibly +3000 hunks) if
#    necessary.
# 4) Most likely broken if reliance on completely automated patching.
# 5) Sloppy tagging.  NVD mixes Patch with Exploit tags.  Patch implies fix;
#    nothing else.
# 6) Links contain references to a commit (or supposed patch) but doesn't say if
#    the commit is introducing the flaw.
# 7) Possibility of re-applying the flaw (due to sloppy tagging or
#    miscommunication of the report by the researcher) if no manual
#    inspection.  Should not use the word Patch because it is
#    dualistic / ambigious.  It's better to use Fix or Flaw instead of Patch.

# Arguments for using automated patching sample
# 1) Faster rate of processing 3000 with JSONs vs 100 by orders of magnitude
#    with manual entry of a new database in around the same timebox.
# 2) It saves time by orders of magnitude.

# Required dependencies
# jq
# bash
# wget or curl
# gunzip
# grep with perl regex support
# html2text
# pcregrep (found in the libpcre package)
# patchutils for filterdiff

# Optional dependencies
# kpatch
# linux kernel with livepatch or kpatch support
# gcc with toolchain to compile the kernel
# patchutils for combinediff

DIR=$(realpath $(dirname "$0"))

ARGS=($@)

NVD_JSON_SCHEMA_VERSION="1.1"
PRODUCT="linux"

G_CACHE_FOLDER="/var/cache/tuxparoni"
G_KERNEL_SOURCE_FOLDER="/usr/src/linux"
G_CUSTOM_PATCHES_DIR="${G_CACHE_FOLDER}/custom_patches"
G_LOGS="${G_CACHE_FOLDER}/logs"
G_KERNEL_TAG=""
G_KERNEL_VER=""
G_KERNEL_SOURCE_BIN=""
G_MIN_YEAR_LIMIT="1999"
G_MAX_YEAR_LIMIT="2020"
G_MIN_YEAR="${G_MIN_YEAR_LIMIT}" # 1999 inclusive is the earliest, 2015 is year linux 4.0 was released
G_MAX_YEAR="${G_MAX_YEAR_LIMIT}"
G_DOWNLOAD_TOOL="wget" # can be wget or curl
G_KERNEL_TIMESTAMP=0
G_HTML2TEXT_WIDTH=320 # to avoid malformed patch
G_TUXPARONI_URL_RESOLVER_HASH=\
"db7d3795b3bb0476be522635946abc36af70c86a6f693952c77237098c76fe4d" # sha256
G_UNATTENDED=0
G_REQ_CLEAR_CACHE=0
G_REQ_FETCH_JSONS=0
G_REQ_FETCH_PATCHES=0
G_REQ_DRY_TEST=0
G_REQ_REPORT=0
G_REQ_APPLY_PATCHES=0
G_REQ_KPATCH_MODULES=0
G_REQ_BUILD_KPATCHES=0
G_PATCH_DEFAULT_OPTIONS="-p1 -F 100"
G_FEEDS_DIR=""
G_JSONS_DIR=""
G_SPLIT_JSONS_DIR=""
G_PATCHES_DIR=""
G_TEMP_DIR="/tmp"
G_ALLOW_UNTAGGED_COMMITS="0" # only from kernel.org or github.com/torvalds,
			     # the commit will mention the cve but not
			     # explicitly say it is a fix or notices a flaw
CVE_ALLOW_RISKY_BACKPORTS=${CVE_ALLOW_RISKY_BACKPORTS:=0}
CVE_DELAY=${CVE_DELAY:=0}
CVE_FIX_REJECT_DISPUTED=${CVE_FIX_REJECT_DISPUTED:=0}
CVE_LANG=${CVE_LANG:=en}
CVE_FIX_TRUST_LEVEL_DEFAULT=$((0x00000001 | 0x00000002 | 0x00010000 \
| 0x00020000 | 0x00040000 \
| 0x00080000 | 0x04000000 | 0x01000000 | 0x01000000 \
| 0x01000000))
CVE_FIX_TRUST_LEVEL=${CVE_FIX_TRUST_LEVEL:=${CVE_FIX_TRUST_LEVEL_DEFAULT}}
G_ALLOW_CRASH_PREVENTION="0"
G_PATCH_META_VERSION="1.1.2"
G_VERSION="1.4.3"
G_MAX_BULK_CONNECTIONS=5 # for bulk transfers
G_MAX_PATCH_CONNECTIONS=100 # should be X KiB bandwidth divided by 3 KiB patch for max throughput, assumed 128KiB
G_CORES=""
G_CURRENT_YEAR="2020"

EOK=0

H_INFO="
$(basename $BASH_SOURCE) v${G_VERSION}
Copyright 2019 Orson Teodoro
License: MIT
"
H_HELP="
$(basename $BASH_SOURCE) <args>

tuxparoni is an automated CVE patcher frontend for the Linux kernel.

Args:
	-d, --delete			deletes caches to start from scratch
	-c, --cache-folder		points to place to store patches, processed jsons, json feeds
	-b, --kernel-bin		points to kernel binary location (e.g. /boot/vmlinuz)
	-s, --kernel-src		points to kernel sources location (e.g. /usr/src/linux)
	-g, --generate-kpatch-modules	generates kpatch modules
	-u,  --unattended		runs unattended
	-au, --allow-untagged		allows untagged commits from kernel.org or from github.com/torvalds
	-acp, --allow-crash-prevention  allows crash prevention commits
	-cj, --cmd-fetch-jsons		fetches, unpacks, filter NVD JSONs
	-cp, --cmd-fetch-patches	fetches, dedupes, tidys commits or patches
	-ct, --cmd-dry-test		does a dry run per CVE patch to determine if sources are vulunerable
	-cr, --cmd-report		shows a mini report per CVE
	-ca, --cmd-apply		applies patches
	-mx, --max-year			sets max year to process
	-mn, --min-year			sets min year to process
	-mpc, --max-patch-connections	sets the maximum patch connections, ${G_MAX_PATCH_CONNECTIONS} is default
	-mbc, --max-bulk-connections	sets the maximum bulk connections, ${G_MAX_BULK_CONNECTIONS} is default
	-t, --temp			sets the temp dir
	--curl				downloads with curl (default)
	--wget				downloads with wget
	-h, --help			prints all the supported commands
	-v, --version			prints version info

Environmental variables:

	CVE_BLACKLIST_FIXES - space seperated string of CVEs to not apply fixes
			      in the form of CVE-2018-14498

	CVE_FIX_TRUST_LEVEL - a whitelist of class of commits or patches to apply
		0x00000001 - kernel.org repos
		0x00000002 - github torvalds
		0x00000004 - reserved
		0x00000008 - module maintainer(s)
		0x00010000 - immediate NVD links
		0x00020000 - indirect NVD links
		0x00040000 - corporate reviewed
		0x00080000 - major distro reviewed
		0x04000000 - major distro team suggested
		0x01000000 - tuxparoni patches
		0x01000000 - FOSS contributor
		0x10000000 - OSS contributor
		0x80000000 - patron
		0x80000000 - user patches
		0x00000000 - disallow untrusted
		0x00000000 - disallow incomplete
		0x00000001 | 0x00000002 | 0x00010000 | 0x00020000 | 0x00040000
			| 0x00080000 | 0x04000000 | 0x01000000 | 0x01000000
			| 0x01000000 - default

	CVE_FIX_REJECT_DISPUTED - rejects applying patches if disputed, 1 or 0,
				  default 0

	CVE_ALLOW_RISKY_BACKPORTS - allows to apply backports that might result
				    in damage or data loss, 1 or 0, default 0

	CVE_DELAY - puts a pause between reporting detailed CVEs, 1 or 0,
		    default 0

	CVE_LANG - en currently only supported for JSON feeds

Patches are licensed under GPL-2 or MIT.  See source of file.

CVE data is under the public domain and obtained and provided by the NVD which
was partly obtained from MITRE.
"

source "${DIR}/tuxparoni-url-resolver"
source "${DIR}/tuxparoni-conflict-resolver"

# data structure agnostic
enqueue_download() {
	local url="${1}"
	local cve_id="${2}"
	noprefix_url=$(echo "${url}" \
		| sed -r -e "s#^(flaw|defunct|banned|info):[ ]*##")
	echo -e "\e[46m\e[30m    Queued   \e[0m ${cve_id} ${noprefix_url}"
	save_download_meta
	echo "${cve_id} ${url}" >> "${G_CACHE_FOLDER}/download_list"
	_reset_tags
}

einfo() {
	echo -e "\e[92m*\e[0m $*"
}

einfow() {
	echo -e "\e[92m*\e[0m $*" | fold -s -w 80
}

ewarn() {
	echo -e "\e[33m*\e[0m $*"
}

ewarnw() {
	echo -e "\e[33m*\e[0m $*" | fold -s -w 80
}

eerror() {
	echo -e "\e[31m*\e[0m $*"
}

eerrorw() {
	echo -e "\e[31m*\e[0m $*" | fold -s -w 80
}

_download_feeds() {
	local P=()
	if [[ "${G_DOWNLOAD_TOOL}" == "curl" ]] ; then
		echo -e "${F}" | xargs -I '{}' -P ${G_MAX_BULK_CONNECTIONS} \
			curl -s -L -C - -o "${G_FEEDS_DIR}/{}" "${rdl_folder}/{}"
	else
		echo -e "${F}" | xargs -I '{}' -P ${G_MAX_BULK_CONNECTIONS} \
			wget -q -c -O "${G_FEEDS_DIR}/{}" "${rdl_folder}/{}"
	fi
	P+=($!)

	einfo "Download feeds in the background in parallel.  Waiting..."

	for p in ${P[@]} ; do
		wait $p 2>/dev/null
	done
}

is_feed_accepted() {
	if [[ "${y}" == "recent" || "${y}" == "modified" ]] ; then
		(( ${G_MAX_YEAR} >= 2007 )) \
			&& return 0
	else
		(( ${G_MIN_YEAR} <= ${y} && ${y} <= ${G_MAX_YEAR} )) \
			&& return 0
	fi
	return 1
}

fetch_feeds() {
	einfo "Fetching feeds"
	local rdl_folder="https://nvd.nist.gov/feeds/json/cve/${NVD_JSON_SCHEMA_VERSION}/"
	local F=""
	local min_year="${G_MIN_YEAR}"
	(( "${G_MIN_YEAR}" <= 2002 )) && min_year="2002"
	for y in $(seq ${min_year} ${G_MAX_YEAR}) recent modified ; do
		is_feed_accepted || continue
		if [[ "${y}" == "${G_MIN_YEAR}" ]] ; then
			F="nvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.json.gz\nnvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.meta"
		else
			F="${F}\nnvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.json.gz\nnvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.meta"
		fi
	done

	local retries=0
	local failed=0
	while true ; do
		failed=0
		_download_feeds
		for y in $(seq ${min_year} ${G_MAX_YEAR}) recent modified ; do
			is_feed_accepted || continue
			if ! gunzip -t "${G_FEEDS_DIR}/nvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.json.gz" ; then
				failed=1
			fi
		done
		if (( ${failed} == 1 && retries <= 3 )) ; then
			retries=$((${retries}+1))
			echo -e \
"\e[41m\e[30m Retrying \e[0m nvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.json.gz (${failed})"
			continue
		elif (( ${failed} == 0  )) ; then
			break
		else
			for y in $(seq ${min_year} ${G_MAX_YEAR}) recent modified ; do
				is_feed_accepted || continue
				if ! gunzip -t "${G_FEEDS_DIR}/nvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.json.gz" ; then
					echo -e
"\e[41m\e[30m  Failed  \e[0m \
${G_FEEDS_DIR}/nvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.json.gz \
failed the integrity test.  Remove it and try again."
					exit 1
				fi
			done
		fi
	done

	for y in $(seq ${min_year} ${G_MAX_YEAR}) recent modified ; do
		is_feed_accepted || continue
		local expected_sz=$(grep -F -e "gzSize" \
			"${G_FEEDS_DIR}/nvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.meta" \
			| cut -f2 -d ":" | sed -r -e "s|[^0-9]||g")
		local x_sz=$(wc -c \
			"${G_FEEDS_DIR}/nvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.json.gz" \
			| cut -f1 -d " ")
		if [[ ! -f "${G_FEEDS_DIR}/nvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.meta" ]] ; then
			echo -e \
"\e[41m\e[30m  Missing \e[0m nvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.meta missing."
		fi
		if [[ ! -f "${G_FEEDS_DIR}/nvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.json.gz" ]] ; then
			echo -e \
"\e[41m\e[30m  Missing \e[0m nvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.meta missing."
		fi
		if [[ "${expected_sz}" != "${x_sz}" && "${expected_h}" != "${x_h}" ]] ; then
			echo -e \
"\e[41m\e[30m  Failed  \e[0m nvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.json.gz.  Clear cache."
			exit 1
		fi
		echo -e "\e[42m\e[30m  Success \e[0m nvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.json.gz"
	done
}

unpack_feeds() {
	einfo "Unpacking feeds"

	local L=""
	local min_year="${G_MIN_YEAR}"
	(( "${G_MIN_YEAR}" <= 2002 )) && min_year="2002"
	for y in $(seq ${min_year} ${G_MAX_YEAR}) recent modified ; do
		is_feed_accepted || continue
		if [[ "${L}" == "${G_MIN_YEAR}" ]] ; then
			L="${y}"
		else
			L="${L}\n${y}"
		fi
	done

	local P=()
	pushd "${G_FEEDS_DIR}" > /dev/null
		echo -e "${L}" | xargs -I '{}' -P ${G_CORES} \
			gunzip -q -f -k \
			nvdcve-${NVD_JSON_SCHEMA_VERSION}-{}.json.gz
		P+=($!)
	popd > /dev/null

	einfo "Unpacking feeds in the background in parallel.  Waiting..."

	for p in ${P[@]} ; do
		wait $p 2>/dev/null
	done

	for y in $(seq ${min_year} ${G_MAX_YEAR}) recent modified ; do
		is_feed_accepted || continue
		local expected_sz=$(grep -F -e "size" \
			"${G_FEEDS_DIR}/nvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.meta" \
			| cut -f2 -d ":" | sed -r -e "s|[^0-9]||g")
		local x_sz=$(wc -c \
			"${G_FEEDS_DIR}/nvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.json" \
			| cut -f1 -d " ")
		local expected_h=$(grep -F -e "sha256" \
			"${G_FEEDS_DIR}/nvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.meta" \
			| cut -f2 -d ":" | sed -r -e "s|[^0-9A-Z]||g")
		expected_h=${expected_h,,}
		local x_h=$(sha256sum \
			"${G_FEEDS_DIR}/nvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.json" \
			| cut -f1 -d " ")
		if [[ "${expected_sz}" != "${x_sz}" && "${expected_h}" != "${x_h}" ]] ; then
			echo -e \
"\e[41m\e[30m Failed \e[0m nvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.json.  Clear cache."
			exit 1
		fi
		echo -e "\e[42m\e[30m Success \e[0m nvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.json"
	done
}

# split to reduce search space and more efficient loading time into jq
split_jsons() {
	einfo "Spliting JSONs"
	local min_year="${G_MIN_YEAR}"
	(( "${G_MIN_YEAR}" <= 2002 )) && min_year="2002"
	for y in $(seq ${min_year} ${G_MAX_YEAR}) recent modified ; do
		is_feed_accepted || continue
		local feed_json="${G_FEEDS_DIR}/nvdcve-${NVD_JSON_SCHEMA_VERSION}-${y}.json"
		einfo "Processing ${y}"

		# reduce the space in one pass
		json_frag=$(tempfile)
		jq "[.CVE_Items[]] \
			| map(select(contains({cve:{description:{description_data:[{value:\"linux kernel\"}]}}}) or \
			contains({cve:{description:{description_data:[{value:\"Linux kernel\"}]}}}) or \
			contains({cve:{description:{description_data:[{value:\"Linux Kernel\"}]}}}) or \
			contains({cve:{description:{description_data:[{value:\"Linux Driver\"}]}}}) or \
			contains({cve:{description:{description_data:[{value:\"Linux driver\"}]}}}) ))" "${feed_json}" \
			> "${json_frag}"

		local C=$(jq ".[].cve.CVE_data_meta.ID" "${json_frag}" | sed -e "s|\"||g")

		Y=$(seq ${G_MIN_YEAR} ${G_MAX_YEAR} | tr "\n" "|" | sed -r -e "s|[\|]$||")
		C=$(echo -e "${C}" | grep -P -e "CVE-(${Y})-[0-9]+")

		local P=()

		echo -e "${C}" | xargs -I '{}' -P ${G_CORES} \
			bash -c "jq \
	'[.[]] | map(select(contains({cve:{CVE_data_meta:{ID:\"{}\"}}})))' \
				'${json_frag}' \
				> '${G_SPLIT_JSONS_DIR}/{}.json'"

		P+=$($!)

		einfo "Splitting in parallel... waiting..."

		for p in ${P[@]} ; do
			wait $p 2>/dev/null
		done

		rm "${json_frag}"

		for c in $C ; do
			if [[ -f "${G_SPLIT_JSONS_DIR}/${c}.json" ]] ; then
				echo -e "\e[42m\e[30m Done \e[0m ${c}"
			else
				echo -e "\e[42m\e[30m Fail \e[0m ${c}"
				exit 1
			fi
		done
	done
	einfo "Done splitting jsons"
}

# use CVSS v3 (four levels) but fall back to v2 (three levels)
get_severity() {
	local severity3=$(jq ".[0].impact.baseMetricV3.cvssV3.baseSeverity" "${json_file}" \
		| sed -e "s|\"||g")
	local severity2=$(jq ".[0].impact.baseMetricV2.severity" "${json_file}" \
		| sed -e "s|\"||g")
	if [[ "${severity3}" != "null" ]] ; then
		severity="${severity3}"
	elif [[ "${severity2}" != "null" ]] ; then
		severity="${severity2}"
	else
		severity="na"
	fi
	echo "${severity}"
}

# context is _process_patch
save_test_meta() {
	# for drytest, report, and filtering drivers/subsystems for
	# custom hardware setups
	local meta_path="${G_PATCHES_DIR}/${dest_fn_base}.meta"
	if [[ -f "${meta_path}" ]] ; then
		#don't clobbler
		return
	fi
	echo "meta_version: ${G_PATCH_META_VERSION}" > "${meta_path}"
	echo "cve_id: ${cve_id}" >> "${meta_path}"
	echo "severity: ${severity}" >> "${meta_path}"
	echo "timestamp: ${timestamp}" >> "${meta_path}"
	[[ -n "${original_url}" ]] \
		&& echo "original_url: ${original_url}" >> "${meta_path}"
	[[ -n "${url}" ]] \
		&& echo "download_url: ${url}" >> "${meta_path}"
	echo "is_patch_processable: ${is_patch_processable}" >> "${meta_path}"
	echo "is_direct_url: ${is_direct_url}" >> "${meta_path}"
	echo "is_disputed: ${is_disputed}" >> "${meta_path}"
	echo "is_substituted_url: ${is_substituted_url}" >> "${meta_path}"
        if [[ -n "${htags}" ]] ; then
                # hidden tags for controlling backports or anything
                echo "htags: ${htags}" >> "${meta_path}"
        fi
	if [[ "${is_patch_processable}" == "1" ]] ; then
		echo "subject: ${subject_raw}" >> "${meta_path}"

		[[ -n "${tag1}" ]] \
			&& echo "tag1: ${tag1}" >> "${meta_path}"
		[[ -n "${tag2}" ]] \
			&& echo "tag2: ${tag2}" >> "${meta_path}"
		[[ -n "${tag3}" ]] \
			&& echo "tag3: ${tag3}" >> "${meta_path}"
		[[ -n "${index}" ]] \
			&& echo "index: ${index}" >> "${meta_path}"
		# can be used to filter for device drivers
		local files=$(cat "${dest}" | grep -P -e "^[+-]{3} " \
			| cut -c 5- | sed -r -e "s|^[a,b]/||g" | uniq \
			| cut -f1 -d $' ' | cut -f1 -d $'\t' | tr "\n" " ")
		echo "files: ${files}" >> "${meta_path}"
		echo "dest_fn_base: ${dest_fn_base}" >> "${meta_path}"
	fi
}

# required preconditions: url (final download url used as the key),
#			  pickled metadata generated from save_download_meta
_process_patch() {
	local cve_id="${2}"
	local htags=""
	local index=""
	local is_direct_url=""
	local is_disputed=""
	local is_substituted_url=""
	local original_url=""
	local request_inspection=""
	local severity=""
	local src=""
	local tag1=""
	local tag2=""
	local tag3=""
	local timestamp=0
	local url="${1}"

	[[ -z "${url}" ]] && return

	restore_download_meta

	if declare -f pre_process_patch > /dev/null ; then
		pre_process_patch "${url}" "${cve_id}" "${src}"
	fi

	local is_patch_processable=0

	# Many links will be marked as "Vendor Advisory" instead of "Patch" at
	# 2016 and earlier.
	# An accepted commit still needs to be validated as a fix patch
	# because the flaws get mixed in the CVE entry.
	if [[ "${request_inspection}" == "1" ]] ; then
		if grep -q -P -i -e \
"(CVE-[0-9]{4}-[0-9]+|fix|validate|sanity check|do not leak|don't leak| cap \
|make sure|use kzalloc|prevent| limit |correct)" "${src}" ; then
			# These are essentially synonyms for fix and limit.

			# Re-review:
			# CVE-2015-6252

			# request_inspection comes from a CVE not explicitly
			# marked Patch for any of its links but contain an
			# official commit from github.com/torvald or kernel.org.
			#
			# The flag will inspect the contents of the
			# commit to determine if it qualifies as a possible fix.
			#

			# The list below lists certain keywords that could be
			# fixes and the the CVEs listed refer a commit
			# containing them.

			# General common keywords are favored to minimize time
			# cost.

			# validate: CVE-2016-5829, CVE-2016-5871, CVE-2015-8569
			# add validation: CVE-2015-8543
			# fix: CVE-2016-5728, CVE-2016-4580, CVE-2016-4578,
			#      CVE-2016-4569
			# fixes: CVE-2016-4568, CVE-2016-3951
			# avoid: CVE-2016-4581
			# sanity check: CVE-2016-3689, CVE-2016-3140,
			#               CVE-2016-3138
			# do not leak: CVE-2016-0823
			# cap the length: CVE-2015-8830
			# cap: CVE-2015-6526
			# initialize: CVE-2015-7613
			# make sure: CVE-2015-5707
			# use kzalloc: CVE-2015-5697
			# closes a bug: CVE-2015-4176
			# check length: CVE-2015-4167
			# to prevent heap overflow: CVE-2015-4001
			# prevents: CVE-2015-3339
			# limit: CVE-2015-2150
			# use correct size: CVE-2015-2042
			# avoid memory overflow: CVE-2009-0065
			# enforce a minimum: CVE-2008-5700
			# remove the buggy: CVE-2008-4307
			# avoid a double fetch: CVE-2017-8831

			is_patch_processable=1
		else
			# to avoid reintroducing the flaw
			:;
		fi
	fi

	if is_patch_good "${src}" ; then
		is_patch_processable=1
	fi

	if is_html2text_permitted "${src}" ; then
		is_patch_processable=1
	fi

	[[ -z "${src}" ]] && is_patch_processable=0

	if [[ "${is_patch_processable}" != "1" ]] ; then
		save_test_meta
		[[ -f "${src}" ]] && rm "${src}"
		return 1
	fi

	len=$(wc -c "${src}" | cut -f1 -d " ")
	if (( ${len} == 0 )) ; then
		is_patch_processable=0
		save_test_meta
		rm "${src}"
		return 1
	else
		if is_html2text_permitted "${src}" ; then
			html2text -width ${G_HTML2TEXT_WIDTH} -utf8 -nobs "${src}" > "${src}.t"
			mv "${src}.t" "${src}"
		fi

		# reorder by timestamp for possibly combinediff
		if is_patch_good "${src}" && grep -P -e "^Date:" "${src}" \
			> /dev/null ; then
			htimestamp=$(grep -P -e "^Date:" "${src}" | head -n 1 \
				| sed -r -e "s|[^a-zA-Z0-9:, +-]||g" \
				| sed -r -e "s|Date:[\t ]*||g" \
				| sed -r -e "s|\([A-Z]+\)||g" -e "s|[ A-Z]+$||")
			if echo "${url}" | cut -f4 -d " " | grep -F -q -e ":" ; then
				htimestamp=$(echo "${htimestamp}" \
					| sed -r -e \
"s|([A-Za-z]+)[,]? ([A-Za-z]+) ([0-9]+) ([0-9]{2}:[0-9]{2}:[0-9]{2}) ([0-9]{4})\
 ([+0-9-]+)|\1, \2 \3 \5 \4 \6|")
			fi
			timestamp=$(date -d "${htimestamp}" +%s)
		fi

		module=$(cat "${src}" \
			| grep -F -e "+++" | tail -n 1 \
			| cut -f2 -d " " \
			| sed -r -e "s|^[ab]/||g" \
			| sed -r -e "s#(.c|.h)\$##g" \
			| sed -r -e "s|[^a-zA-Z0-9_]|-|g")
		subject=$(get_sanitized_commit_subject \
			"${src}")
		subject_raw=$(get_sampled_commit_subject \
			"${src}")
		# 255 filename limit so strink as much as possible
		#local dest_fn="${cve_id}--${PRODUCT}--${module}--${subject}.patch" # preferred

		# the naming scheme was selected for admins/endusers to pick
		# based on hardware of the device.

		# module: hints subsystem and driver.
		# severity: CVSS v3 but falls back to v2.
		# timestamp: unix timestamp of the commit.  Useful for
		#            reordering patches.
		# tags: additional metadata to filter for the device/arch.
		tags=""
		[[ -n "${tag1}" ]] && tags+="-${tag1}"
		[[ -n "${tag2}" ]] && tags+="-${tag2}"
		[[ -n "${tag3}" ]] && tags+="-${tag3}"
		[[ -n "${index}" ]] && tags+="-${index}"
		tags="${tags,,}"
		tags=$(echo "${tags}" | sed -e "s|\.|_|g")
		local dest_fn_base=\
"${timestamp}-${cve_id}-${severity,,}-${module,,}${tags}"
		local dest_fn="${dest_fn_base}.patch"
		local dest="${G_PATCHES_DIR}/${dest_fn}"
		mv "${src}" "${dest}"

		if cat "${dest}" \
			| grep -F -e 'Diff 1 not found or parse error; hopeless!'
		then
			is_patch_processable=0
			save_test_meta
			rm "${dest}"
			return 1
		fi

		if ! is_patch_good "${dest}" ; then
			is_patch_processable=0
			save_test_meta
			rm "${dest}"
			return 1
		fi
		save_test_meta
	fi
	return 0
}

fetch_patches() {
	queue_urls
	download_urls
}

queue_urls() {
	cat /dev/null > "${G_CACHE_FOLDER}/download_list"
	touch "${G_CACHE_FOLDER}/already_downloaded"

	local min_year="${G_MIN_YEAR}"
	for json_file in $(ls "${G_SPLIT_JSONS_DIR}"/*.json) ; do
		local is_patchable=0
		local exclude_binary=0

		local cve_id=$(basename "${json_file}" | sed -e "s|.json||g")
		local cve_year=$(echo "${cve_id}" | cut -f2 -d '-')

		(( ${G_MIN_YEAR} <= ${cve_year} \
			&& ${cve_year} <= ${G_MAX_YEAR} )) \
			|| continue

		local desc=$(jq \
			'.[0].cve.description.description_data[0].value' \
			"${json_file}" \
			| sed -e "s|\"||g")
		local is_disputed=0
		local request_inspection=0

		local timestamp=0
		# put the unknown dates at the end of the queue; also
		# reduce code to consider leap year
		timestamp=$(($(date -d "1/1/$((${cve_year}+1))" +%s)-1))
		local severity=$(get_severity)

		local dest_fn_base="${timestamp}-${cve_id}-${severity,,}"
		[[ "${desc}" =~ DISPUTED ]] && is_disputed=1

		local reference_data_length=$(jq \
		'.[0].cve.references.reference_data | length' "${json_file}")

		is_patchable=0
		for i in $(seq 0 $((${reference_data_length}-1))) ; do
			local tags=$(jq \
				'.[].cve.references.reference_data[].tags[]' \
				"${json_file}")
			if [[ "${tags}" =~ (Patch|Vendor Advisory) ]] ; then
				is_patchable=1
			fi
		done

		for i in $(seq 0 $((${reference_data_length}-1))) ; do
			local is_direct_url="0"
			local is_substituted_url="0"
			local original_url=$(jq \
				".[].cve.references.reference_data[${i}].url" \
				"${json_file}" \
				| sed -e "s|\"||g" | head -n 1)
			if [[ "${is_patchable}" == "1" ]] ; then
				local tags=$(jq \
				".[].cve.references.reference_data[${i}].tags" \
						"${json_file}")
				if [[ "${tags}" =~ (Patch|Vendor Advisory) ]] ; then
					get_patch_direct_link "${original_url}" "${cve_id}"
					download_url="${url}"
					[[ "${original_url}" == "${download_url}" ]] \
						&& is_direct_url="1"
					local l1=$(echo "${original_url}" | sed -r -e "s|^http[s]?://||")
					local l2=$(echo "${download_url}" | sed -r -e "s|^http[s]?://||")
					[[ "${l1}" == "${l2}" ]] \
						&& is_direct_url="1"
					queue_patch "${download_url}" "${cve_id}"
					continue
				fi
			fi

			if [[ "${G_ALLOW_UNTAGGED_COMMITS}" == "1" \
				&& ( "${original_url}" =~ ("github.com/torvalds"|"kernel.org") ) \
				&& ( "${original_url}" =~ (;h=[0-9a-z]{40}|"/patch/") ) ]]
			then
				request_inspection=1
				get_patch_direct_link "${original_url}" "${cve_id}"
				download_url="${url}"
				[[ "${original_url}" == "${download_url}" ]] \
					&& is_direct_url="1"
				local l1=$(echo "${original_url}" | sed -r -e "s|^http[s]?://||")
				local l2=$(echo "${download_url}" | sed -r -e "s|^http[s]?://||")
				[[ "${l1}" == "${l2}" ]] \
					&& is_direct_url="1"
				queue_patch "${download_url}" "${cve_id}"
				continue
			fi

			if [[ "${is_patchable}" == "0" ]] ; then
				echo -e "\e[101m\e[30m Unpatchable \e[0m ${cve_id}"
				is_patch_processable=0
				save_test_meta
			fi
		done
	done
	einfo "Queuing urls done"
}

download_urls() {
	cat "${G_CACHE_FOLDER}/download_list" | sort \
		| uniq > "${G_CACHE_FOLDER}/download_list.t"
	mv "${G_CACHE_FOLDER}/download_list"{.t,}

	einfo "Downloading patches"

	local P=()
	local min_year="${G_MIN_YEAR}"

	local thread_count=0

	while IFS= read -r line ; do
		local cve_id=$(echo "${line}" | cut -f 1 -d $' ')
		local cve_year=$(echo "${cve_id}" | cut -f2 -d '-')

		(( ${G_MIN_YEAR} <= ${cve_year} \
			&& ${cve_year} <= ${G_MAX_YEAR} )) \
			|| continue

		local url=$(echo "${line}" | cut -f 2- -d $' ')

		grep -q -F -e "${line}" "${G_CACHE_FOLDER}/already_downloaded" \
			&& echo -n "c" && continue

		[[ "${url}" =~ (^info|^defunct|^banned|^flaw) ]] && continue
		local dl_meta="${G_TEMP_DIR}"/$(echo "${cve_id}:${url}" \
			| sha1sum | cut -f1 -d " ")
		local src=$(grep -P -e "^src:" "${dl_meta}" | cut -f2 -d $' ')

		# can silence but need to create a shell
		if [[ "${G_DOWNLOAD_TOOL}" == "curl" ]] ; then
			curl -s -L -C - -o "${src}" "${url}" & >/dev/null
		else
			wget -b -q -c -O "${src}" "${url}" >/dev/null
		fi
		P+=($!)
		thread_count=$(((${thread_count}+1) % ${G_MAX_PATCH_CONNECTIONS}))

		if (( ${G_MAX_PATCH_CONNECTIONS} == 0 )) ; then
			for p in ${P[@]} ; do
				wait $p 2>/dev/null
			done
			P=()
		fi

		echo -n "."
	done < "${G_CACHE_FOLDER}/download_list"

	echo ""
	einfo "Waiting for all download threads to complete"

	for p in ${P[@]} ; do
		wait $p 2>/dev/null
	done

	P=()

	einfo "Evaluating patches for validity, cleaning, etc"

	while IFS= read -r line ; do
		local cve_id=$(echo "${line}" | cut -f 1 -d $' ')
		local cve_year=$(echo "${cve_id}" | cut -f2 -d '-')

		(( ${G_MIN_YEAR} <= ${cve_year} \
			&& ${cve_year} <= ${G_MAX_YEAR} )) \
			|| continue

		local url=$(echo "${line}" | cut -f 2- -d $' ')

		grep -q -F -e "${line}" "${G_CACHE_FOLDER}/already_downloaded" \
			&& echo -n "c" && continue

		( _process_patch "${url}" "${cve_id}" ) &
		P+=($!)
		echo -n "."
	done < "${G_CACHE_FOLDER}/download_list"

	echo ""

	einfo "Waiting for all evaluations to complete"

	for p in ${P[@]} ; do
		wait $p 2>/dev/null
	done

	einfo "Evaluations done"

	while IFS= read -r line ; do
		local cve_id=$(echo "${line}" | cut -f 1 -d $' ')
		local cve_year=$(echo "${cve_id}" | cut -f2 -d '-')

		(( ${G_MIN_YEAR} <= ${cve_year} \
			&& ${cve_year} <= ${G_MAX_YEAR} )) \
			|| continue

		if ! grep -q -F -e "${line}" "${G_CACHE_FOLDER}/already_downloaded" ; then
			echo "${line}" >> "${G_CACHE_FOLDER}/already_downloaded"
		fi

		local url=$(echo "${line}" | cut -f 2- -d $' ')

		local meta_path=""

		meta_path=$(grep -l -r -e "${url}" \
			"${G_PATCHES_DIR}"/*${cve_id}*.meta 2>/dev/null)

		if [[ ! -f "${meta_path}" ]] ; then
			if [[ "${url}" =~ ^info ]] ; then
				url=$(echo "${url}" | sed -r -e "s|^info:[ ]*||")
				echo -e "\e[46m\e[30m     Info    \e[0m ${cve_id} ${url}"
			else
				url=$(echo "${url}" \
					| sed -r -e "s#^(flaw|defunct|banned):[ ]*##")
				echo -e "\e[41m\e[30m   Rejected  \e[0m ${cve_id} ${url}"
			fi
			continue
		fi

		[[ -z "${meta_path}" ]] && continue
		local is_patch_processable=$(grep -F \
			-e "is_patch_processable" "${meta_path}" \
			| cut -f 2 -d $' ')
		if [[ "${is_patch_processable}" == "1" ]] ; then
			local dest_fn=$(grep -F \
				-e "dest_fn_base" "${meta_path}" \
				| cut -f 2 -d $' ').patch
			echo -e "\e[42m\e[30m   Accepted  \e[0m ${cve_id} ${dest_fn} ${url}"
		else
			echo -e "\e[41m\e[30m   Rejected  \e[0m ${cve_id} ${url}"
		fi
	done < "${G_CACHE_FOLDER}/download_list"
}

get_sanitized_commit_subject() {
	local fn="${1}"
	echo $(cat "${fn}" | grep -P -e "^Subject" | head -n 1 \
		| sed -e "s|Subject: ||g" | sed -r -e "s|:|_|" -e "s| |_|g" \
		-e "s|\(\)|fn|g" -e "s|[^A-Za-z0-9._-]||g" -e "s|\.|_|g")
}

get_sampled_commit_subject() {
	local fn="${1}"
	echo $(cat "${fn}" | grep -P -e "^Subject" | head -n 1 \
		| sed -e "s|Subject: ||g")
}

_reset_tags() {
	unset tag1
	unset tag2
        unset tag3
        unset htags
	unset index
}

ban_commit() {
	url="${1}"

	# openwall says introduced flaw, but nvd marks it as patch
	if [[ "${url}" =~ "129a72a0d3c8e139a04512325384fe5ac119e74" ]] ; then
		url="flaw:  ${url}"
	elif [[ "${url}" =~ secunia\.com ]] ; then
		url="defunct:  ${url}"
	elif [[ "${url}" =~ "codeaurora.org/security-bulletin/" \
		|| "${url}" =~ "android.com/security/bulletin/" \
		|| "${url}" =~ "redhat.com/support/errata/RHSA-" \
		|| "${url}" =~ "redhat.com/errata/RHSA-" \
		|| "${url}" =~ "novell.com/linux/security/advisories/" \
		|| "${url}" =~ "debian.org/security/"[0-9]{4}"/dsa" \
		|| "${url}" =~ "kernel.org".*"ChangeLog-"[0-9.]+$ \
		|| "${url}" =~ "show_bug.cgi" \
		|| "${url}" =~ "bugs.launchpad.net" ]] ; then
		# scraping from any bugzilla or dynamic page directly
		# is dangerous.  An attacker has carte blanche freedom
		# to post flaw if topic is not locked and rogue
		# comment is not removed immediately.
		url="info:  ${url}"
	fi
}

is_broken_url() {
	if [[ "${url}" =~ ("stable/linux.git"|"torvalds/linux.git") ]] ; then
		:;
	else
		if [[ "${G_DOWNLOAD_TOOL}" == "curl" ]] ; then
			curl -o /dev/null -s -D - \
				| grep -q -F -e "404 Not found" \
				&& return 0
		else
			wget -O /dev/null "${url}" 2>&1 \
				| grep -q -F -e "404 Not found" \
				&& return 0
		fi
	fi
	return 1
}

# provides links directly to raw patch
get_patch_direct_link() {
	url="${1}"
	local cve_id="${2}"

	if [[ "${url}" =~ ^"http://" ]] ; then
		# prevent man in the middle (MITM) attack
		url=$(echo "${url}" | sed -r -e "s|^http://|https://|g")
		# todo: do the same with ftp
	fi

	ban_commit "${url}"

	if [[ "${url}" =~ "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git" ]] ; then
		local h=$(echo "${url}" | grep -o -P -e "([a-z0-9]+)\$")
		url="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/patch/?id=${h}"
		is_direct_url="1"
	elif [[ "${url}" =~ "git.kernel.org/linus/" ]] ; then
		local h=$(echo "${url}" | grep -o -P -e "([a-z0-9]+)\$")
		url="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/patch/?id=${h}"
		is_direct_url="1"
	elif [[ "${url}" =~ www\.kernel\.org \
		&& "${url}" =~ "hb=" ]]
	then
		local h=$(echo "${url}" | sed -r -e "s|.*hb=([0-9a-z]{40}).*|\1|g")
		local p=$(echo "${url}" | cut -f2 -d "?" \
			| cut -f1 -d ";" | sed -e "s|p=||" \
			| sed -r -e "s|linux-[0-9.]*.git|linux.git|g")
		if is_broken_url ; then
			url="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/patch/?id=${h}"
			is_substituted_url="1"
		else
			url="https://git.kernel.org/pub/scm/${p}/patch/?id=${h}"
			is_direct_url="1"
		fi
	elif [[ ( "${url}" =~ git\.kernel\.org || \
		  "${url}" =~ www\.kernel\.org ) \
		&& ( "${url}" =~ ";a=commitdiff;" \
		  || "${url}" =~ ";a=commit;") \
		&& ! ( "${url}" =~ "hb=" ) ]]
	then
		local p=$(echo "${url}" | cut -f2 -d "?" \
			| cut -f1 -d ";" | sed -e "s|p=||" \
			| sed -r -e "s|linux-[0-9.y]*.git|linux.git|g")
		local h=$(echo "${url}" | grep -o -P -e ";h=([a-z0-9]+)" \
			| sed -e "s|;h=||")
		if is_broken_url ; then
			url="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/patch/?id=${h}"
			is_substituted_url="1"
		else
			url="https://git.kernel.org/pub/scm/${p}/patch/?id=${h}"
			is_direct_url="1"
		fi
	elif [[ "${url}" =~ "/commit/".*(\&|\?)"id="[0-9a-z]+ ]] ; then
		url=$(echo "${url}" | sed -e "s|/commit/|/patch/|")
		is_direct_url="1"
	elif [[ "${url}" =~ lore\.kernel\.org ]] ; then
		url=$(echo "${url}" | sed -r -e "s|/T/(#u)?|/|g")
		if [[ "${url}" =~ /$ ]] ; then
			url="${url}raw"
		else
			url="${url}/raw"
		fi
		is_direct_url="1"
	elif [[ "${url}" =~ patchwork\.kernel\.org ]] ; then
		# testcase http://patchwork.kernel.org/patch/36649/
		if [[ "${G_DOWNLOAD_TOOL}" == "curl" ]] ; then
			url=$(curl -L -s -o /dev/null -w %{url_effective} \
				"${url}")
			url="${url}mbox"
		else
			local log=$(wget -O /dev/null "${url}" 2>&1)
			if echo "${log}" | grep -q -P -e "http.*\[following\]" ; then
				# prevent from getting a nothing result
				url=$(echo "${log}" \
					| grep -P -e "http.*\[following\]" \
					| sed -e "s|Location: ||" \
					-e "s|\[following\]||g" \
					| tail -n 1 | sed -r -e "s|[ ]+$||g")
			fi
			url="${url}mbox"
		fi
		is_direct_url="1"
	elif [[ "${url}" =~ patchwork ]] ; then
		url="${url}mbox"
		is_direct_url="1"
	elif [[ "${url}" =~ lkml\.org ]] ; then
		url=$(echo "${url}" | sed -e "s|/lkml/|/lkml/diff/|g" \
			| echo "$(cat -)/1")
		is_direct_url="1"
	elif [[ "${url}" =~ github\.com && ! ( "${url}" =~ \.patch$ ) \
		&& ! ( "${url}" =~ \.md$ ) \
		&& ! ( "${url}" =~ "/issues/" ) ]] ; then
		url="${url}.patch"
		is_direct_url="1"
	elif [[ "${url}" =~ salsa\.debian\.org \
		&& "${url}" =~ "/commit/" ]]
	then
		url="${url}.patch"
		is_direct_url="1"
	elif [[ "${url}" =~ marc.info ]] ; then
		url="${url}&q=mbox"
		is_direct_url="1"
	elif [[ "${url}" =~ "securityfocus.com/bid" ]] ; then
		url="${url}/solution"
		is_direct_url="1"
	fi

	# autofix broken urls


	fix_direct_download "${url}" "${cve_id}"
}

is_patch_good() {
	local f="${1}"

	[[ -f "${f}" ]] || return 1

	if pcregrep -s -M "^--- .*\n^\+\+\+ .*\n" "${f}" 2>/dev/null 1>/dev/null ; then
		return 0
	else
		return 1
	fi
}

is_html2text_permitted() {
	local f="${1}"
	local is_html=0
	[[ -f "${f}" ]] || return 1
	cat "${f}" | grep -q -F -e '</html>' 2>/dev/null 1>/dev/null && is_html=1
	if [[ "${is_html}" == "1" ]] && is_patch_good "${f}" ; then
		return 0
	else
		return 1
	fi
}

# We need data independence for multithreading.
# Store all data associated with a url as a (pickled) object.
# Store all state information related to this data.
# When we have data independence, we can process out of order
# if blocking I/O for both downloading and _process_patch.
# context is enqueue_download
save_download_meta() {
	# ${cve_id}:${url} is used to ensure that ${src} doesn't
	# get removed if it used multiple times per different cve
	# When _process_patch gets called,
	# _process_patch will move $src to $dest or remove it
	# if it is invalid.
	dl_meta="${G_TEMP_DIR}"/$(echo "${cve_id}:${url}" | sha1sum | cut -f1 -d " ")

	if [[ -f "${dl_meta}" ]] ; then
		# don't clobber
		return
	fi

	echo "cve_id: ${cve_id}" > "${dl_meta}"
	echo "download_url: ${url}" >> "${dl_meta}"
	[[ -n "${htags}" ]] \
		&& echo "htags: ${htags}" >> "${dl_meta}"
	[[ -n "${index}" ]] \
		&& echo "index: ${index}" >> "${dl_meta}"
	echo "is_direct_url: ${is_direct_url}" >> "${dl_meta}"
	echo "is_disputed: ${is_disputed}" >> "${dl_meta}"
	echo "is_substituted_url: ${is_substituted_url}" >> "${dl_meta}"
	echo "original_url: ${original_url}" >> "${dl_meta}"
	echo "request_inspection: ${request_inspection}" >> "${dl_meta}"
	echo "severity: ${severity}" >> "${dl_meta}"
	[[ -n "${tag1}" ]] \
		&& echo "tag1: ${tag1}" >> "${dl_meta}"
	[[ -n "${tag2}" ]] \
		&& echo "tag2: ${tag2}" >> "${dl_meta}"
	[[ -n "${tag3}" ]] \
		&& echo "tag3: ${tag3}" >> "${dl_meta}"
	echo "src: ${src}" >> "${dl_meta}"
	echo "timestamp: ${timestamp}" >> "${dl_meta}"
	src="${dl_meta}.data"
	cat /dev/null > "${src}"
}

# context is _process_patch
restore_download_meta() {
	dl_meta="${G_TEMP_DIR}"/$(echo "${cve_id}:${url}" | sha1sum | cut -f1 -d " ")
	cve_id=$(grep -F -e "cve_id" "${dl_meta}" | cut -f2 -d $' ')
	grep -q -F -e "htags" "${dl_meta}" && \
		htags=$(grep -F -e "htags" "${dl_meta}" | cut -f2 -d $' ')
	grep -q -F -e "index" "${dl_meta}" && \
		index=$(grep -F -e "index" "${dl_meta}" | cut -f2 -d $' ')
	is_direct_url=$(grep -F -e "is_direct_url" "${dl_meta}" | cut -f2 -d $' ')
	is_disputed=$(grep -F -e "is_disputed" "${dl_meta}" | cut -f2 -d $' ')
	is_substituted_url=$(grep -F -e "is_substituted_url" "${dl_meta}" | cut -f2 -d $' ')
	original_url=$(grep -F -e "original_url" "${dl_meta}" | cut -f2 -d $' ')
	request_inspection=$(grep -F -e "request_inspection" "${dl_meta}" | cut -f2 -d $' ')
	severity=$(grep -F -e "severity" "${dl_meta}" | cut -f2 -d $' ')
	src=$(grep -P -e "^src" "${dl_meta}" | cut -f2 -d $' ')
	grep -q -F -e "tag1" "${dl_meta}" && \
		tag1=$(grep -F -e "tag1" "${dl_meta}" | cut -f2 -d $' ')
	grep -q -F -e "tag2" "${dl_meta}" && \
		tag2=$(grep -F -e "tag2" "${dl_meta}" | cut -f2 -d $' ')
	grep -q -F -e "tag3" "${dl_meta}" && \
		tag3=$(grep -F -e "tag3" "${dl_meta}" | cut -f2 -d $' ')
	timestamp=$(grep -F -e "timestamp" "${dl_meta}" | cut -f2 -d $' ')
	url=$(grep -F -e "download_url" "${dl_meta}" | cut -f2 -d $' ')
	rm "${dl_meta}"
}

# skips non patchfiles
queue_patch() {
	local url="${1}"
	local cve_id="${2}"

	if [[ "${exclude_binary}" == "0" && "${url}" =~ \.zip$|\.xz$|\.gz$|\.bz2$ ]] ; then
		# don't download patchsets or kernel incremental patches
		return 1
	elif [[ "${url}" =~ "source.android.com/security/bulletin" ]] ; then
		return 1
	fi

	_reset_tags

	# multipatch
	if queue_multipatch "${url}" "${cve_id}" ; then
		return 0
	fi

	enqueue_download "${url}" "${cve_id}"
	return 0
}

# this only reports based on configuration data
report_cves_json_only() {
	# todo
	:;
}

# this only reports if the patch was applied and doesn't consider CVEs without patches at the moment
report_cves_patches_only() {
	pushd "${G_KERNEL_SOURCE_FOLDER}" > /dev/null

	local min_year="${G_MIN_YEAR}"
	for json_file in $(ls "${G_SPLIT_JSONS_DIR}"/*.json) ; do
		#einfo "Processing ${json_file}"

		local cve_id=$(basename "${json_file}" | sed -e "s|.json||g")
		local cve_year=$(echo "${cve_id}" | cut -f2 -d "-")

		(( ${G_MIN_YEAR} <= ${cve_year} \
			&& ${cve_year} <= ${G_MAX_YEAR} )) \
			|| continue

		local desc=$(jq '.[].cve.description.description_data[0].value' "${json_file}" \
				| sed -e "s|\"||g")
		local severity=$(get_severity)

		local vulnerability_class="0" # 0=none, 1=possibly, 2=0-day, no known patch exists
		local t_files=$(tempfile)
		ls "${G_PATCHES_DIR}"/*${cve_id}*.patch > "${t_files}" 2>/dev/null
		if [[ "$?" == "0" ]] ; then
			while IFS= read -r p ; do
				patch --dry-run ${G_PATCH_DEFAULT_OPTIONS} \
					-i "${p}" 2> /dev/null 1> /dev/null
				if [[ "$?" == "0" ]] ; then
					vulnerability_class="1"
				fi
			done < "${t_files}"
		else
			vulnerability_class="2"
		fi
		[[ -f "${t_files}" ]] && rm "${t_files}"

		if (( "${vulnerability_class}" > 0 )) ; then
			einfo "${cve_id}"
			einfow "Description:  ${desc}"
			einfo "NIST NVD:  https://nvd.nist.gov/vuln/detail/${cve_id}"
			einfo "MITRE:  https://cve.mitre.org/cgi-bin/cvename.cgi?name=${cve_id}"
			if [[ "${severity}" == "CRITICAL" ]] ; then
				einfo "Severity:  \e[100mCRITICAL\e[0m"
			elif [[ "${severity}" == "HIGH" ]] ; then
				einfo "Severity:  \e[41mHIGH\e[0m"
			elif [[ "${severity}" == "MEDIUM" ]] ; then
				einfo "Severity:  \e[43m\e[30mMedium\e[0m"
			elif [[ "${severity}" == "LOW" ]] ; then
				einfo "Severity:  \e[44m\e[30mLow\e[0m"
			else
				einfo "Severity:  N/A"
			fi
			if [[ "${vulnerability_class}" == "2" ]] ; then
				einfo "Vulnerable: yes, no known patch exists"
			elif [[ "${vulnerability_class}" == "1" ]] ; then
				einfo "Vulnerable: possibly yes, false positive possible"
			else
				einfo "Unknown vulnerability class"
			fi
			einfo ""
		else
			einfo "${cve_id} skipped"
		fi
	done
	einfo \
"Skipped:  This means that it could be a bad patch, bad backport, bad forward \
port."
	einfo \
"Vulnerable: possibly yes, false positive possible:  This means that the \
patch applied without problems."
	einfo \
"Vulnerable: yes, no known patch exists:  This means no patch file was \
provided to resolve the CVE by the security community or by the module \
maintainer."
	popd > /dev/null
}

drytest_cves() {
	einfo "Drytesting"
	local t_files=$(tempfile)
	ls "${G_PATCHES_DIR}"/*.meta > "${t_files}" 2>/dev/null
	if [[ "$?" != 0 ]] ; then
		eerror "No patches found to dry run"
		exit 1
	fi
	local c=0
	local total=0
	local bad_patches=0
	local bad_ports=0
	pushd "${G_KERNEL_SOURCE_FOLDER}" > /dev/null
	local t_out=$(tempfile)
	local displayed_kernel_release=0
	local min_year="${G_MIN_YEAR}"
	cat /dev/null > "${G_LOGS}/drytest"
	while IFS= read -r m ; do
		total=$((total+1))
		local p="${G_PATCHES_DIR}"/$(basename "${m}" | sed -r -e "s|\.patch$|.meta|g")
		local cve_id=$(grep -F -e "cve_id" "${m}" | cut -f2 -d $' ')
		local cve_year=$(echo "${cve_id}" | cut -f2 -d "-")

		(( ${G_MIN_YEAR} <= ${cve_year} \
			&& ${cve_year} <= ${G_MAX_YEAR} )) \
			|| continue

		local patch_timestamp=$(grep -F -e "timestamp" "${m}" | cut -f2 -d $' ')
		local is_patch_processable=$(grep -F -e "is_patch_processable" "${m}" | cut -f2 -d $' ')
		if (( ${patch_timestamp} >= ${G_KERNEL_TIMESTAMP} \
			&& ${displayed_kernel_release} == 0 ))
		then
			echo -e "\e[44m\e[30m Kernel Released \e[0m"
			displayed_kernel_release=1
		fi
		cat /dev/null > "${t_out}"
		local is_failed_at=0
		local is_file_missing=0
		local is_malformed=0
		local is_previously_applied=0
		local is_fuzzy_succeeded=0
		local is_garbage=0
		if [[ "${is_patch_processable}" == "1" ]] ; then
			echo -e $(patch --dry-run ${G_PATCH_DEFAULT_OPTIONS} \
				-i "${p}" 2>&1) \
				| grep -q -F -e "succeeded at" && is_fuzzy_succeeded=1
			echo -e $(patch --dry-run ${G_PATCH_DEFAULT_OPTIONS} \
				-i "${p}" 2>&1) \
				| grep -q -F -e "FAILED at" && is_failed_at=1
			echo -e $(patch --dry-run ${G_PATCH_DEFAULT_OPTIONS} \
				-i "${p}" 2>&1) \
				| grep -q -F -e "can't find file" && is_file_missing=1
			echo -e $(patch --dry-run ${G_PATCH_DEFAULT_OPTIONS} \
				-i "${p}" 2>&1) \
				| grep -q -F -e "malformed" && is_malformed=1
			echo -e $(patch --dry-run ${G_PATCH_DEFAULT_OPTIONS} \
				-i "${p}" 2>&1) \
				| grep -q -F -e "Reversed (or previously applied)" \
				&& is_previously_applied=1
			echo -e $(patch --dry-run ${G_PATCH_DEFAULT_OPTIONS} \
				-i "${p}" 2>&1) \
				| grep -q -F -e "Only garbage was found in the patch input." \
				&& is_garbage=1

			# todo?
			#read error : Is a directory

			patch --dry-run ${G_PATCH_DEFAULT_OPTIONS} \
				-i "${p}" 2> /dev/null 1> /dev/null
			local is_vulnerable="$?"
		fi
		if [[ "${is_previously_applied}" == "1" \
			&& "${is_fuzzy_succeeded}" != "1" \
			&& "${is_failed_at}" != "1" \
			&& "${is_file_missing}" != "1" \
			&& "${is_malformed}" != "1" ]]
		then
			echo -e "\e[42m\e[30m   Passed   \e[0m ${cve_id}"
			echo -e "\e[42m\e[30m   Passed   \e[0m ${cve_id}" \
			>> "${G_LOGS}/drytest"
		elif [[ "${is_failed_at}" == "1" || "${is_file_missing}" == "1" ]] ; then
			local h=$(sha256sum "${p}" | cut -f1 -d " ")
			bad_ports=$((${bad_ports}+1))
			echo -e \
"\e[100m\e[97m  Bad port  \e[0m ${cve_id}"
			echo -e \
"\e[100m\e[97m  Bad port  \e[0m ${cve_id} sha256=${h} path=${p}" \
			>> "${G_LOGS}/drytest"
		elif [[ "${is_malformed}" == "1" || "${is_garbage}" == "1" ]] ; then
			local h=$(sha256sum "${p}" | cut -f1 -d " ")
			bad_patches=$((${bad_patches}+1))
			echo -e \
"\e[100m\e[97m  Bad patch \e[0m ${cve_id}"
			echo -e \
"\e[100m\e[97m  Bad patch \e[0m ${cve_id} sha256=${h} path=${p}" \
			>> "${G_LOGS}/drytest"
		elif [[ "${is_vulnerable}" == "0" || "${is_patch_processable}" == "0" ]] ; then
			echo -e "\e[41m Vulnerable \e[0m ${cve_id}"
			c=$((c+1))
		elif [[ "${is_previously_applied}" == 1 ]] ; then
			local h=$(sha256sum "${p}" | cut -f1 -d " ")
			# Try to avoid duplicate hunk problem
			echo -e \
"\e[100m\e[97m  Rejected  \e[0m ${cve_id}"
			echo -e \
"\e[100m\e[97m  Rejected  \e[0m ${cve_id} (dupe hunk problem avoidance) sha256=${h} path=${p}" \
			>> "${G_LOGS}/drytest"
		else
			eerror \
"uncaught case is_failed_at=${is_failed_at} is_file_missing=${is_file_missing} \
is_malformed=${is_malformed} is_previously_applied=${is_previously_applied} \
is_fuzzy_succeeded=${is_fuzzy_succeeded} is_vulnerable=${is_vulnerable}"
			eerror "Patch: ${p}"
			rm "${t_out}"
			rm "${t_files}"
			exit 1
		fi
	done < "${t_files}"
	rm "${t_out}"
	popd > /dev/null
	rm "${t_files}"
	local hunks=$(grep -P -e "^@@" "${G_PATCHES_DIR}"/* | wc -l)
	einfo \
"${c} vulnerabilities detected, ${total} total patches tested, ${bad_patches} \
bad patches, ${bad_ports} bad backports or forwardports patches, ${hunks} \
hunks involved"
}

is_patch_authorized() {
	local patch_path="${1}"
	local patch_meta_path="${G_PATCHES_DIR}"/$(basename "${patch_path}" | sed -r -e "s|\.patch$|.meta|g")
	local download_url=$(grep -F -e "download_url:" "${patch_meta_path}" | cut -f2 -d $' ')
	local is_direct_url=$(grep -F -e "is_direct_url:" "${patch_meta_path}" | cut -f2 -d $' ')
	local is_disputed=$(grep -F -e "is_disputed:" "${patch_meta_path}" | cut -f2 -d $' ')
	local is_substituted_url=$(grep -F -e "is_substituted_url:" "${patch_meta_path}" | cut -f2 -d $' ')
	local original_url=$(grep -F -e "original_url:" "${patch_meta_path}" | cut -f2 -d $' ')
	for c in ${CVE_BLACKLIST_FIXES} ; do
		if [[ "${c}" == "${cve_id}" ]] ; then
			return 1
		fi
	done

	# See also https://github.com/orsonteodoro/oiledmachine-overlay/blob/088556e40afa1d0de742683649ac6dc0bb8d0c3c/eclass/ot-kernel-cve.eclass
	# for how these hashes were obtained

	if (( ${CVE_FIX_TRUST_LEVEL} & 0x00000001 )) ; then
		# kernel.org repos
		if [[ "${download_url}" =~ "kernel.org" ]] ; then
			return 0
		fi
	fi
	if (( ${CVE_FIX_TRUST_LEVEL} & 0x00000002 )) ; then
		# github.com/torvalds
		if [[ "${download_url}" =~ "github.com/torvalds" ]] ; then
			return 0
		fi
	fi
	if (( ${CVE_FIX_TRUST_LEVEL} & 0x00000008 )) ; then
		# module maintainer(s)
		# this flag concerns patches that are WIP or under code review

		# todo, requires manual inspection if the person submitting is
		# the module maintainer on the copyright or a daily contributor
		# on the repo.
		# no special checks are required if commit is obtained from the
		# official repo.
		# if the patch is not from official repo like patchwork then
		# special vetting is required.
		# the problem is the attacker can pretend to be a credible
		# source, via social engineering attack.
		if [[ "${download_url}" =~ 3d94a4a8373bf5f45cf5f939e88b8354dbf2311b ]] ; then
			# for CVE-2019-14895
			return 0
		fi
	fi
	if (( ${CVE_FIX_TRUST_LEVEL} & 0x00010000 )) ; then
		# immediate NVD links
		# considers followed resolved links as direct
		if [[ "${is_direct_url}" == "1" ]] ; then
			return 0
		fi
	fi
	if (( ${CVE_FIX_TRUST_LEVEL} & 0x00020000 )) ; then
		# indirect NVD links
		# todo, requires tuxparoni-url-resolver, meta changes
		if [[ "${download_url}" =~ b60fe990c6b07ef6d4df67bc0530c7c90a62623a ]] ; then
			# for CVE-2019-14821
			return 0
		elif [[ "${is_substituted_url}" == "1" ]] ; then
			# these are substituted manually
			return 0
		fi
	fi
	if (( ${CVE_FIX_TRUST_LEVEL} & 0x00040000 )) ; then
		# corporate reviewed
		# todo, requires manual inspection of verified accounts feature
		if [[ "${download_url}" =~ 3d94a4a8373bf5f45cf5f939e88b8354dbf2311b ]] ; then
			# for CVE-2019-14895
			return 0
		fi
	fi
	if (( ${CVE_FIX_TRUST_LEVEL} & 0x04000000 )) ; then
		# major distro team suggested
		# todo, requires manual inspected
		if [[ "${download_url}" =~ a10d9a71bafd3a283da240d2868e71346d2aef6f ]] ; then
			# for CVE-2007-3732
			return 0
		fi
	fi
	if (( ${CVE_FIX_TRUST_LEVEL} & 0x01000000 )) ; then
		# FOSS contributor, tuxparoni patches
		# todo, requires manual vetting of the originator of the author
		# design standards for this flag:
		# patch author has project from github, sourceforge, gitlab, etc.
		# projects released under open licenses

		if [[ "${is_tuxparoni_patch}" == "1" ]] ; then
			return 0
		fi

		# todo: compare against whitelist authors
	fi
	if (( ${CVE_FIX_TRUST_LEVEL} & 0x10000000 )) ; then # OSS contributor
		if [[ "${download_url}" == "https://lkml.org/lkml/diff/2019/9/9/487/1" ]] ; then
			# for CVE-2019-16229 ... CVE_2019_16233
			return 0
		fi
		# todo, requires manual vetting of the originator of the author
		# design standards for this flag:
		# patch author has authored non-free code that is widely used
	fi
	if (( ${CVE_FIX_TRUST_LEVEL} & 0x80000000 )) ; then # patron, user patches, your patches
		[[ "${is_user_patch}" == "1" ]] \
			&& return 0
	fi

	# unimplemented yet
	if (( ${CVE_FIX_TRUST_LEVEL} & 0x00000004 )) ; then
		# reserved
		:;
	fi
	if (( ${CVE_FIX_TRUST_LEVEL} & 0x00080000 )) ; then
		# major distro reviewed
		# todo, requires manual inspection
		# none from old file
		:;
	fi
	return 1
}

# true / triage patch
# idea is to apply then do micro changes on the broken hunk
tpatch() {
	local patch_path="${patch_path}"
	patch ${G_PATCH_DEFAULT_OPTIONS} -i "${patch_path}" || true
	echo -e "\e[100m\e[97m   Triaged  \e[0m ${cve_id} ${patch_patch}"
}

# exit if broken
epatch() {
	local patch_path="${patch_path}"
	patch ${G_PATCH_DEFAULT_OPTIONS} -i "${patch_path}"
	if [[ "$?" != "0" ]] ; then
		echo -e "\e[100m\e[97m   Failed   \e[0m ${cve_id} ${patch_patch}"
		exit 1
	else
		echo -e "\e[100m\e[97m   Applied  \e[0m ${cve_id}"
	fi
}

# version tests
# x < y
is_x_lt_y() {
	local x="${1}"
	local y="${2}"
	local result=$(echo -e "${x}\n${y}" | sort -V | tr "\n" " " | cut -f1 -d $' ')
	if [[ "${result}" == "${x}" ]] ; then
		return 0
	else
		return 1
	fi
}

is_x_le_y() {
	local x="${1}"
	local y="${2}"
	local result=$(echo -e "${x}\n${y}" | sort -V | tr "\n" " " | cut -f1 -d $' ')
	if [[ "${result}" == "${x}" || "${x}" == "${y}" ]] ; then
		return 0
	else
		return 1
	fi
}

is_x_gt_y() {
	local x="${1}"
	local y="${2}"
	local result=$(echo -e "${x}\n${y}" | sort -V | tr "\n" " " | cut -f1 -d $' ')
	if [[ "${result}" != "${x}" ]] ; then
		return 0
	else
		return 1
	fi
}

is_x_ge_y() {
	local x="${1}"
	local y="${2}"
	local result=$(echo -e "${x}\n${y}" | sort -V | tr "\n" " " | cut -f1 -d $' ')
	if [[ "${result}" == "${x}" || "${x}" == "${y}" ]] ; then
		return 0
	else
		return 1
	fi
}

# Template function, it should be defined in tuxparoni-conflict-resolver as
# conflict_resolver with custom entries like below per each sha256sum.
# The failed dry run will spit out a sha256 of the failed backport which can be used
# to backport.

# This will apply to custom or fowardport patches not just backport patches.
# You may use this to apply more that one patch per failed patch.
# You may use this to apply a sequence of patches or unreported patches.
# You will use the patch_hash as like a hook.
# You may use this to skip patches.

# We don't distribute custom backport patches at the moment because of licensing,
# but it left to custom kernel maintainers.

conflict_resolver_default() {
	local cve_id="${1}"
	local patch_path="${2}" # original patch which can be used to revert or
				# partially apply it
	local patch_meta_path=$(echo "${patch_path}" \
		| sed -r -e "s|\.patch$|.meta|g")
	local original_url=$(cat "${patch_meta_path}" \
		| grep -F -e "original_url:" | cut -f2 -d $' ')
	local download_url=$(cat "${patch_meta_path}" \
		| grep -F -e "download_url:" | cut -f2 -d $' ')
	local patch_hash=$(sha256sum "${patch_path}" \
		| cut -f1 -d " ")
	local alterative_patch=""
	if [[ "${patch_hash}" == "blah" ]] ; then
		# path to patch
		p="${G_CUSTOM_PATCHES_DIR}/blah.patch"
		is_user_patch="1"
	fi
}

apply_patches() {
	einfo "Applying CVE patches"
	local t_files=$(tempfile)
	ls "${G_PATCHES_DIR}"/*.patch > "${t_files}" 2>/dev/null
	if [[ "$?" != "0" ]] ; then
		eerror "Missing patches to apply"
		exit 1
	fi
	local c=0
	local total=0
	local bad_patches=0
	local bad_ports=0
	pushd "${G_KERNEL_SOURCE_FOLDER}" > /dev/null
	local t_out=$(tempfile)
	local displayed_kernel_release=0
	local min_year="${G_MIN_YEAR}"
	while IFS= read -r p ; do
		total=$((total+1))
		local cve_id=$(basename "${p}" | cut -f2-4 -d "-")
		local cve_year=$(echo "${cve_id}" | cut -f2 -d "-")

		(( ${G_MIN_YEAR} <= ${cve_year} \
			&& ${cve_year} <= ${G_MAX_YEAR} )) \
			|| continue

		local patch_timestamp=$(basename "${p}" | cut -f1 -d "-")
		if (( ${patch_timestamp} >= ${G_KERNEL_TIMESTAMP} \
			&& ${displayed_kernel_release} == 0 )) ; then
			echo -e "\e[44m\e[30m Kernel Released \e[0m"
			displayed_kernel_release=1
		fi
		cat /dev/null > "${t_out}"

		local is_user_patch="0"
		local is_tuxparoni_patch="0"
		if declare -f conflict_resolver > /dev/null ; then
			conflict_resolver "${cve_id}" "${p}"
		fi

		local is_failed_at=0
		local is_file_missing=0
		local is_malformed=0
		local is_previously_applied=0
		local is_fuzzy_succeeded=0
		local is_garbage=0
		echo -e $(patch --dry-run ${G_PATCH_DEFAULT_OPTIONS} \
			-i "${p}" 2>&1) \
			| grep -q -F -e "succeeded at" && is_fuzzy_succeeded=1
		echo -e $(patch --dry-run ${G_PATCH_DEFAULT_OPTIONS} \
			-i "${p}" 2>&1) \
			| grep -q -F -e "FAILED at" && is_failed_at=1
		echo -e $(patch --dry-run ${G_PATCH_DEFAULT_OPTIONS} \
			-i "${p}" 2>&1) \
			| grep -q -F -e "can't find file" && is_file_missing=1
		echo -e $(patch --dry-run ${G_PATCH_DEFAULT_OPTIONS} \
			-i "${p}" 2>&1) \
			| grep -q -F -e "malformed" && is_malformed=1
		echo -e $(patch --dry-run ${G_PATCH_DEFAULT_OPTIONS} \
			-i "${p}" 2>&1) \
			| grep -q -F -e "Reversed (or previously applied)" \
			&& is_previously_applied=1
		echo -e $(patch --dry-run ${G_PATCH_DEFAULT_OPTIONS} \
			-i "${p}" 2>&1) \
			| grep -q -F -e "Only garbage was found in the patch input." \
			&& is_garbage=1
		patch --dry-run ${G_PATCH_DEFAULT_OPTIONS} \
			-i "${p}" 2> /dev/null 1> /dev/null
		local is_vulnerable="$?" # 0 is true
		if [[ "${is_previously_applied}" == "1" \
			&& "${is_fuzzy_succeeded}" != "1" \
			&& "${is_failed_at}" != "1" \
			&& "${is_file_missing}" != "1" \
			&& "${is_malformed}" != "1" ]]
		then
			echo -e "\e[42m\e[30m   Passed   \e[0m ${cve_id}"
		elif [[ "${is_failed_at}" == "1" || "${is_file_missing}" == "1" ]] ; then
			bad_ports=$((${bad_ports}+1))
			echo -e "\e[100m\e[97m  Bad port  \e[0m ${cve_id} file=${p}"
		elif [[ "${is_malformed}" == "1" || "${is_garbage}" == "1" ]] ; then
			bad_patches=$((${bad_patches}+1))
			echo -e "\e[100m\e[97m  Bad patch \e[0m ${cve_id} file=${p}"
		elif [[ "${is_vulnerable}" == "0" ]] ; then
			if is_patch_authorized "${p}" ; then
				echo -e "\e[44m\e[30m   Applied  \e[0m ${cve_id}"
				patch ${G_PATCH_DEFAULT_OPTIONS} \
					-i "${p}" 2> /dev/null 1> /dev/null
			else
				echo -e \
"\e[100m\e[97m  Rejected  \e[0m ${cve_id} (unauthorized) ${p}"
			fi
			c=$((c+1))
		elif [[ "${is_previously_applied}" == 1 ]] ; then
			# Try to avoid duplicate hunk problem that
			# could cause overflows, underflows, crashes
			echo -e \
"\e[100m\e[97m  Rejected  \e[0m ${cve_id} (dupe hunk problem avoidance) file=${p}"
		else
			eerror "uncaught case"
			rm "${t_out}"
			rm "${f_files}"
			exit 1
		fi
	done < "${t_files}"
	rm "${t_out}"
	popd > /dev/null
	rm "${t_files}"
	local hunks=$(grep -P -e "^@@" "${G_PATCHES_DIR}"/* | wc -l)
	einfo \
"${c} vulnerabilities detected, ${total} total patches tested, ${bad_patches} \
bad patches, ${bad_ports} bad backports or forwardports patches, ${hunks} \
hunks involved"
}

generate_kpatch_modules() {
	local t_files=$(tempfile)
	ls "${G_PATCHES_DIR}"/*.patch > "${t_files}" 2>/dev/null
	if [[ "$?" != "0" ]] ; then
		eerror "Missing patches to generate kpatch modules"
		exit 1
	fi
	pushd "${G_KERNEL_SOURCE_FOLDER}" > /dev/null
	while IFS= read -r p ; do
		local cve_id=$(echo "${p}" | sed -r -e "s|(CVE-[0-9]+-[0-9]+).*|\1|")
		local p_abs_path="${p}"
		einfo "Testing patch at ${p_path}"
		patch -f --dry-run ${G_PATCH_DEFAULT_OPTIONS} \
			-i "${p_abs_path}" 2>&1 > /dev/null
		if [[ "$?" == "0" ]] ; then
			ewarn \
"Kernel may be vulnerable to ${cve_id}.  Generating live patch."
			kpatch-build -t "${G_KERNEL_SOURCE_BIN}" "${p_abs_path}"
		else
			einfo "Kernel may not be vulnerable to ${cve_id}."
		fi
		echo -e "\n"
	done < "${t_files}"
	popd > /dev/null
	rm "${t_files}"
}

get_tag() {
	if [[ -d "${G_KERNEL_SOURCE_FOLDER}" ]] ; then
		local version=$(grep -F -e "VERSION" \
			"${G_KERNEL_SOURCE_FOLDER}/Makefile" | head -n1 \
			| cut -f2 -d "=" | sed -e "s| ||g")
		local patchlevel=$(grep -F -e "PATCHLEVEL" \
			"${G_KERNEL_SOURCE_FOLDER}/Makefile" | head -n1 \
			| cut -f2 -d "=" | sed -e "s| ||g")
		local sublevel=$(grep -F -e "SUBLEVEL" \
			"${G_KERNEL_SOURCE_FOLDER}/Makefile" | head -n1 \
			| cut -f2 -d "=" | sed -e "s| ||g")
		local extraversion=$(grep -F -e "EXTRAVERSION" \
			"${G_KERNEL_SOURCE_FOLDER}/Makefile" | head -n1 | \
			cut -f2 -d "=" | sed -e "s| ||g")
		echo "${version}.${patchlevel}.${sublevel}${extraversion}"
	fi
}

setup_globals() {
	G_FEEDS_DIR="${G_CACHE_FOLDER}/feeds"
	G_JSONS_DIR="${G_CACHE_FOLDER}/jsons"
	G_SPLIT_JSONS_DIR="${G_CACHE_FOLDER}/split_jsons"
	G_PATCHES_DIR="${G_CACHE_FOLDER}/patches"
	G_CUSTOM_PATCHES_DIR="${G_CACHE_FOLDER}/custom_patches"
	G_LOGS="${G_CACHE_FOLDER}/logs"
	G_CORES=$(cat /proc/cpuinfo | grep processor | wc -l)
	G_CURRENT_YEAR=$(date +%Y)
}

_merged_mkdir() {
	local path="${1}"
	mkdir -p "${path}"
	chmod 0750 "${path}"
	chown root:root "${path}"
}

setup_cache() {
	_merged_mkdir "${G_CACHE_FOLDER}"
	_merged_mkdir "${G_CUSTOM_PATCHES_DIR}"
	_merged_mkdir "${G_FEEDS_DIR}"
	_merged_mkdir "${G_JSONS_DIR}"
	_merged_mkdir "${G_LOGS}"
	_merged_mkdir "${G_PATCHES_DIR}"
	_merged_mkdir "${G_SPLIT_JSONS_DIR}"
}

clear_cache() {
	einfo "Wiping cache"
	einfo "G_FEEDS_DIR=${G_FEEDS_DIR}"
	rm -rf "${G_FEEDS_DIR}"
	rm -rf "${G_JSONS_DIR}"
	rm -rf "${G_LOGS}"
	rm -rf "${G_PATCHES_DIR}"
	rm -rf "${G_SPLIT_JSONS_DIR}"

	rm -f "${G_CACHE_FOLDER}/already_downloaded"
	rm -f "${G_CACHE_FOLDER}/download_list"
}

arg_parse() {
	#echo "ARGS=${ARGS[@]}"
	while (( ${#ARGS[@]} > 0 )) ; do
		arg="${ARGS[0]}"
		ARGS=("${ARGS[@]:1}")
		case "$arg" in
			-acp|--allow-crash-prevention)
				G_ALLOW_CRASH_PREVENTION="1"
				;;
			-au|--allow-untagged)
				G_ALLOW_UNTAGGED_COMMITS="1"
				;;
			-b|--kernel-bin)
				G_KERNEL_SOURCE_BIN="${ARGS[0]}"
				ARGS=("${ARGS[@]:1}")
				;;
			-c|--cache-folder)
				G_CACHE_FOLDER="${ARGS[0]}"
				ARGS=("${ARGS[@]:1}")
				;;
			-ca|--cmd-apply)
				G_REQ_APPLY_PATCHES="1"
				;;
			-cj|--cmd-fetch-jsons)
				G_REQ_FETCH_JSONS="1"
				;;
			--curl)
				G_DOWNLOAD_TOOL="curl"
				;;
			-cp|--cmd-fetch-patches)
				G_REQ_FETCH_PATCHES="1"
				;;
			-cr|--cmd-report)
				G_REQ_REPORT="1"
				;;
			-ct|--cmd-dry-test)
				G_REQ_DRY_TEST="1"
				;;
			-d|--delete)
				G_REQ_CLEAR_CACHE="1"
				;;
			-g|--generate-kpatch-modules)
				G_REQ_BUILD_KPATCHES="1"
				;;
			-h|--help|-help)
				echo "${H_HELP}"
				exit ${EOK}
				;;
			-mbc|--max-bulk-connections)
				G_MAX_BULK_CONNECTIONS="${ARGS[0]}"
				ARGS=("${ARGS[@]:1}")
				;;
			-mpc|--max-patch-connections)
				G_MAX_PATCH_CONNECTIONS="${ARGS[0]}"
				ARGS=("${ARGS[@]:1}")
				;;
			-mn|--min-year)
				G_MIN_YEAR="${ARGS[0]}"
				ARGS=("${ARGS[@]:1}")
				;;
			-mx|--max-year)
				G_MAX_YEAR="${ARGS[0]}"
				ARGS=("${ARGS[@]:1}")
				;;
			-s|--kernel-src)
				G_KERNEL_SOURCE_FOLDER="${ARGS[0]}"
				ARGS=("${ARGS[@]:1}")
				;;
			-t|--temp)
				G_TEMP_DIR="${ARGS[0]}"
				ARGS=("${ARGS[@]:1}")
				;;
			-u|--unattended)
				G_UNATTENDED="1"
				;;
			--version|-version|-ver)
				echo "${H_INFO}"
				exit ${EOK}
				;;
			--wget)
				G_DOWNLOAD_TOOL="wget"
				;;
			*)
		esac
	done
}

kpatch_combine() {
	#todo
	:;
}

get_kernel_timestamp() {
	local ver="${1}"

	local sublevel=$(echo "${ver}" | cut -f3 -d '.')
	if [[ "${sublevel}" == "0" ]] ; then
		ver=$(echo "${ver}" | cut -f1-2 -d '.')
	fi

	local t=$(tempfile)
	# strange bug
	if ! wget -q -O "${t}" \
"https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=v${ver}"
	then
		eerror "Failed network, retry."
		exit 1
	fi
	html2text -width ${G_HTML2TEXT_WIDTH} -utf8 -nobs "${t}" > "${t}.t"
	mv "${t}.t" "${t}"
	local ret=$(cat "${t}" | grep -P -e "^committer" | grep -o -P -e \
"[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2} (\-|\+|[ ])*[0-9]{4}")
	date -d "${ret}" +%s
	rm "${t}"
}

dedupe_patches_by_hash() {
	einfo "Running deduper"
	local t1=$(tempfile)
	sha1sum "${G_PATCHES_DIR}"/* | sort | cut -c 1-40 > "${t1}"
	local t2=$(tempfile)
	sha1sum "${G_PATCHES_DIR}"/* | sort | cut -c 1-40 | uniq > "${t2}"
	D=$(diff -urp "${t1}" "${t2}" | grep -P -e "^-" | cut -c 2- | tail -n +2 | uniq)

	for d in ${D} ; do
		F=($(sha1sum "${G_PATCHES_DIR}"/* | grep -F -e "${d}" | cut -f3 -d " "))
		F=(${F[@]:1})
		while (( ${#F[@]} > 0 )) ; do
			f="${F[0]}"
			rm "${f}" && \
			einfo "\e[100m\e[30m Removed \e[0m ${f}"
			F=("${F[@]:1}")
		done
	done
	rm "${t1}" "${t2}"
}

# this will usually process any patch that didn't get obtained from a
# git repo.
tidy_patches() {
	einfo "Running patch tidy"

	P=$(ls "${G_PATCHES_DIR}"/*.patch)
	if [[ "$?" != "0" ]] ; then
		return
	fi

	for p in ${P} ; do
		if ! ( cat "${p}" | head -n 1 | tail -n +1 \
			| grep -q -P -e "^From [0-9a-z]{40} " )
		then
			einfo "Tidying ${p}"
			local t=$(tempfile)
			filterdiff "${p}" > "${t}"
			mv "${t}" "${p}"
		fi
	done
}

review_envars() {
	einfo "CVE_BLACKLIST_FIXES=${CVE_BLACKLIST_FIXES}"
	einfo "CVE_FIX_TRUST_LEVEL=${CVE_FIX_TRUST_LEVEL}"
	einfo "CVE_FIX_REJECT_DISPUTED=${CVE_FIX_REJECT_DISPUTED}"
	einfo "CVE_ALLOW_RISKY_BACKPORTS=${CVE_ALLOW_RISKY_BACKPORTS}"
	einfo "CVE_DELAY=${CVE_DELAY}"
	einfo "CVE_LANG=${CVE_LANG}"

	if [[ "${G_UNATTENDED}" == "0" ]] ; then
		einfo "Accept the above environmental variables? (y/n)"
		read X
		if [[ "${X,,}" != "y" ]] ; then
			einfo "Rejected environmental variables.  Exiting."
			exit 0
		fi
	fi
}

validate_envars_and_globals() {
	# sanitize
	G_MAX_BULK_CONNECTIONS=$(echo "${G_MAX_BULK_CONNECTIONS}" \
		| sed -r -e "s|[^0-9]||g")
	G_MAX_PATCH_CONNECTIONS=$(echo "${G_MAX_PATCH_CONNECTIONS}" \
		| sed -r -e "s|[^0-9]||g")
	G_MIN_YEAR=$(echo "${G_MIN_YEAR}" | sed -r -e "s|[^0-9]||g")
	G_MAX_YEAR=$(echo "${G_MAX_YEAR}" | sed -r -e "s|[^0-9]||g")

	[[ "${CVE_ALLOW_RISKY_BACKPORTS}" == 1 \
		|| "${CVE_ALLOW_RISKY_BACKPORTS}" == 0 ]] \
		|| CVE_FIX_REJECT_DISPUTED=0
	[[ "${CVE_DELAY}" == 1 || "${CVE_DELAY}" == 0 ]] \
		|| CVE_DELAY=0
	[[ "${CVE_FIX_REJECT_DISPUTED}" == 1 \
		|| "${CVE_FIX_REJECT_DISPUTED}" == 0 ]] \
		|| CVE_FIX_REJECT_DISPUTED=0
	[[ "${CVE_LANG}" == "en" ]] || CVE_LANG="en"
	if [[ ! -d "${G_CACHE_FOLDER}" ]] ; then
		eerror \
"G_CACHE_FOLDER does not point to a directory.  Exiting..."
		exit 1
	fi
	if [[ "${G_CACHE_FOLDER}" == "/" ]] ; then
		eerror \
"G_CACHE_FOLDER points to a root.  Dangerous.  Exiting..."
		exit 1
	fi
	if [[ ! -d "${G_KERNEL_SOURCE_FOLDER}" ]] ; then
		eerror \
"G_KERNEL_SOURCE_FOLDER does not point to a directory.  Exiting..."
		exit 1
	fi
	if (( "${G_MIN_YEAR}" < ${G_MIN_YEAR_LIMIT} )) ; then
		eerror "G_MIN_YEAR must be >= ${G_MIN_YEAR_LIMIT}"
		exit 1
	fi
	if (( "${G_MAX_YEAR}" > ${G_MAX_YEAR_LIMIT} )) ; then
		eerror "G_MAX_YEAR must be <= ${G_MAX_YEAR_LIMIT}"
		exit 1
	fi
	if (( "${G_MAX_YEAR}" < "${G_MIN_YEAR}" )) ; then
		eerror \
"G_MAX_YEAR must be >= G_MIN_YEAR and between ${G_MIN_YEAR_LIMIT}-\
${G_MAX_YEAR_LIMIT}"
		exit 1
	fi
	if (( "${G_MIN_YEAR}" > "${G_MAX_YEAR}" )) ; then
		eerror \
"G_MIN_YEAR must be >= G_MAX_YEAR and between ${G_MIN_YEAR_LIMIT}-\
${G_MAX_YEAR_LIMIT}"
		exit 1
	fi
	if [[ "${G_DOWNLOAD_TOOL}" == "curl" \
		|| "${G_DOWNLOAD_TOOL}" == "wget" ]] ; then
		:;
	else
		eerror "G_DOWNLOAD_TOOL must be curl or wget"
		exit 1
	fi
	if [[ -n "${CVE_BLACKLIST_FIXES}" ]] ; then
		for c in ${CVE_BLACKLIST_FIXES} ; do
			if echo "${c}" \
			 | grep -P -e "^CVE-[0-9]{4}-[0-9]+$"
			then
				:;
			else
				eerror \
"${c} is invalid for a CVE ID.  It must be all caps and in CVE-YYYY-XXXX format"
				exit 1
			fi
		done
	fi
	if [[ ! -d "${G_TEMP_DIR}" ]] ; then
		eerror "The temp directory is not set or doesn't exist.  See --help."
		exit 1
	fi
}

clear_downloads() {
	[[ -f "${G_CACHE_FOLDER}/download_list" ]] \
		&& rm "${G_CACHE_FOLDER}/download_list"
}

_dependency_fail() {
	if [[ -n "${2}" ]] ; then
		eerror "You are missing the ${1} command from the package ${2}."
	else
		eerror "You are missing the ${1} command."
	fi
	exit 1
}

check_dependencies() {
	if [[ "${G_DOWNLOAD_TOOL}" == "wget" ]] ; then
		which wget >/dev/null || _dependency_fail "wget"
	else
		which wget >/dev/null || _dependency_fail "curl"
	fi
	which jq >/dev/null || _dependency_fail "jq"
	which pcregrep >/dev/null || _dependency_fail "pcregrep" "libpcre"
	which grep >/dev/null || _dependency_fail "grep"
	if echo "lol" | ./grep -P "lol" 2>&1 \
		| grep -q -F -e "support for the -P option is not compiled" ; then
		eerror "Compile/download grep with Perl regular expression support from your package manager."
		exit 1
	fi
	which html2text >/dev/null || _dependency_fail "html2text"
	which gunzip >/dev/null || _dependency_fail "gunzip" "gzip"
	which filterdiff >/dev/null || _dependency_fail "filterdiff" "patchutils"
	which xargs >/dev/null || _dependency_fail "xargs" "findutils"
	local bash_version=$(echo "${BASH_VERSION}" \
		| grep -o -E -e "[0-9\.]+" | head -n 1)
	if is_x_lt_y "${bash_version}" "4.0" ; then
		eerror "You need Bash >= 4.0"
		exit 1
	fi
}

main() {
	check_dependencies
	einfo "Project is still WIP"

	arg_parse
	review_envars
	setup_globals
	validate_envars_and_globals

	local X_VERSION
	local X_PATCH_META_VERSION
	local X_TUXPARONI_URL_RESOLVER_HASH
	local need_wipe=0
	einfo "CPU Cores=${G_CORES}"

	if [[ -f "${G_CACHE_FOLDER}/status" ]] ; then
		X_VERSION=$(\
			grep -F -e "tuxparoni_version:" "${G_CACHE_FOLDER}/status" \
			| cut -f2 -d $' ')
		X_PATCH_META_VERSION=$(\
			grep -F -e "meta_version:" "${G_CACHE_FOLDER}/status" \
			| cut -f2 -d $' ')
		X_TUXPARONI_URL_RESOLVER_HASH=$(\
			grep -F -e "resolver_hash:" "${G_CACHE_FOLDER}/status" \
			| cut -f2 -d $' ')

		[[ "${X_VERSION}" != "${G_VERSION}" ]] \
			&& need_wipe=1
		[[ "${X_PATCH_META_VERSION}" != "${G_PATCH_META_VERSION}" ]] \
			&& need_wipe=1
		[[ "${X_TUXPARONI_URL_RESOLVER_HASH}" \
			!= "${G_TUXPARONI_URL_RESOLVER_HASH}" ]] \
			&& need_wipe=1
	else
		need_wipe=1
	fi

	if [[ "${G_REQ_CLEAR_CACHE}" == "1" || "${need_wipe}" == "1" ]] ; then
		clear_cache
	fi

	clear_downloads

	# this is used to wipe the cache if necessary
	echo "tuxparoni_version: ${G_VERSION}" > "${G_CACHE_FOLDER}/status"
	echo "meta_version: ${G_PATCH_META_VERSION}" >> "${G_CACHE_FOLDER}/status"
	echo "resolver_hash: ${G_TUXPARONI_URL_RESOLVER_HASH}" >> "${G_CACHE_FOLDER}/status"

	G_KERNEL_TAG=$(get_tag)
	G_KERNEL_VER=$(echo ${G_KERNEL_TAG} | cut -f1 -d "-")
	einfo "Kernel tag: ${G_KERNEL_TAG}"

	G_KERNEL_TIMESTAMP=$(get_kernel_timestamp "${G_KERNEL_VER}")
	einfo "Kernel release timestamp: ${G_KERNEL_TIMESTAMP}   "\
$(date -d "@${G_KERNEL_TIMESTAMP}")

	if [[ "${G_TUXPARONI_URL_RESOLVER_HASH}" \
		!= $(sha256sum "${DIR}/tuxparoni-url-resolver" \
			| cut -f1 -d " ") ]]
	then
		eerror "Resolver module is not authorized."
		exit 1
	fi

	setup_cache
	[[ "${G_REQ_FETCH_JSONS}" == "1" ]] && fetch_feeds
	[[ "${G_REQ_FETCH_JSONS}" == "1" ]] && unpack_feeds
	[[ "${G_REQ_FETCH_JSONS}" == "1" ]] && split_jsons
	[[ "${G_REQ_FETCH_PATCHES}" == "1" ]] && fetch_patches
	[[ "${G_REQ_FETCH_PATCHES}" == "1" ]] && dedupe_patches_by_hash
	[[ "${G_REQ_FETCH_PATCHES}" == "1" ]] && tidy_patches

	[[ "${G_REQ_KPATCH_MODULES}" == "1" ]] && kpatch_combine

	[[ "${G_REQ_REPORT}" == "1" ]] && report_cves_patches_only
	[[ "${G_REQ_REPORT}" == "1" ]] && report_cves_json_only
	[[ "${G_REQ_DRY_TEST}" == "1" ]] && drytest_cves
	[[ "${G_REQ_BUILD_KPATCHES}" == "1" ]] && generate_kpatch_modules
	[[ "${G_REQ_APPLY_PATCHES}" == "1" ]] && apply_patches

	exit 0
}

main
